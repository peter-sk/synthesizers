* save_pretrained and from_pretrained for statedicts
* split pipeline
* wait for syntheval to fix target_col==None and update package afterwards
* use benchmark module from syntheval?
* save data also to xlsx etc.
* R synthpop as backends
-- old - check if still applicable
* output class for all pipelines with optional return_dict = True
* standardized list of supported metrics (supported by any backend)
* standardized list of supported generation methods (supported by any backend)
* output format for evaluation pipeline
* accumulation of multiple outputs
* implement holdout for training and synthesis
* select and combine evaluation backends automatically for given list of metrics?
* select generation backend automatically for given generation method
* why is num_cols not used in distance_to_closest_record? why does test.py fail for fast_eval?
* make syntheval file output optional
* syntheval plots etc as PIL images or something?
* parameter for only ouput or full dict incl. mode, original data etc.
* push_to_hub method on models a la https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_dataset.py
* push_to_hub method on datasets
* other backends?
* how to handle datasetdicts?
* should dataset be the default output?
* save function (via ensure_format and datasetdict.save_to_disk...)
